{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Quantization Overview:** (Do Not Run The Code In This Cell)"
      ],
      "metadata": {
        "id": "kzh73Cl992jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Original word vector for 'cat': [ 0.45 -0.32  0.12  0.91 -0.56]\n",
        "\n",
        "Quantized word vector (using centroids):\n",
        "[ 0.445 -0.56   0.445  0.445 -0.56 ]\n",
        "\n",
        "Cluster centroids (lookup table):\n",
        "[ 0.445 -0.56   0.12 ]\n",
        "\n",
        "Indices of the quantized word vector (pointing to centroids):\n",
        "[0 1 0 0 1]\n",
        "\n",
        "Dequantized word vector for 'cat': [ 0.445 -0.56   0.445  0.445 -0.56 ]"
      ],
      "metadata": {
        "id": "XrNX0QE09XXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VPTQ:**"
      ],
      "metadata": {
        "id": "W9P9Lhlh8Kuz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKCADO1c3Dtq",
        "outputId": "dc0662fd-1368-48a2-bcf8-d114e21cdff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model before quantization:\n",
            "Epoch 1, Loss: 0.6754213478416204\n",
            "Epoch 2, Loss: 0.6464092042297125\n",
            "Epoch 3, Loss: 0.6132115349173546\n",
            "Epoch 4, Loss: 0.5718646757304668\n",
            "Epoch 5, Loss: 0.5307172844186425\n",
            "Epoch 6, Loss: 0.49559501465409994\n",
            "Epoch 7, Loss: 0.46544008888304234\n",
            "Epoch 8, Loss: 0.44852482434362173\n",
            "Epoch 9, Loss: 0.43288429733365774\n",
            "Epoch 10, Loss: 0.41632851865142584\n",
            "\n",
            "Applying vector quantization to model weights...\n",
            "Training the model after quantization:\n",
            "Epoch 1, Loss: 0.41042427998036146\n",
            "Epoch 2, Loss: 0.3944588592275977\n",
            "Epoch 3, Loss: 0.38029393134638667\n",
            "Epoch 4, Loss: 0.3777327071875334\n",
            "Epoch 5, Loss: 0.364715694449842\n",
            "Epoch 6, Loss: 0.3652488263323903\n",
            "Epoch 7, Loss: 0.3485293942503631\n",
            "Epoch 8, Loss: 0.34981080051511526\n",
            "Epoch 9, Loss: 0.336352757178247\n",
            "Epoch 10, Loss: 0.3363899355754256\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_moons\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 1. Simple Neural Network in PyTorch\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# 2. Generate a simple dataset (two moons)\n",
        "def generate_data():\n",
        "    X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
        "    X, y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "    return X, y\n",
        "\n",
        "# 3. Train function for the simple neural network\n",
        "def train(model, loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(loader)}\")\n",
        "\n",
        "# 4. Function to perform Vector Quantization (VQ)\n",
        "def vector_quantization(weights, num_clusters=8):\n",
        "    shape = weights.shape\n",
        "    # Flatten the weights to 2D for clustering\n",
        "    weights_flat = weights.reshape(-1, 1)\n",
        "\n",
        "    # Use KMeans for vector quantization\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(weights_flat)\n",
        "\n",
        "    # Replace weights with the nearest centroid\n",
        "    quantized_weights = kmeans.cluster_centers_[kmeans.labels_]\n",
        "\n",
        "    # Reshape back to the original weight shape\n",
        "    return quantized_weights.reshape(shape)\n",
        "\n",
        "# 5. Function to apply quantization to the model\n",
        "def apply_quantization(model, num_clusters=8):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            if len(param.shape) > 1:  # Only quantize weights (not biases)\n",
        "                quantized_weights = vector_quantization(param.numpy(), num_clusters)\n",
        "                param.copy_(torch.tensor(quantized_weights))\n",
        "\n",
        "# 6. Main script\n",
        "def main():\n",
        "    # Generate dataset and create DataLoader\n",
        "    X, y = generate_data()\n",
        "    dataset = TensorDataset(X, y)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    model = SimpleNN()\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"Training the model before quantization:\")\n",
        "    # Train the model for a few epochs\n",
        "    train(model, loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "    # Apply vector quantization to the weights\n",
        "    print(\"\\nApplying vector quantization to model weights...\")\n",
        "    apply_quantization(model, num_clusters=8)\n",
        "\n",
        "    print(\"Training the model after quantization:\")\n",
        "    # Re-train the quantized model to see the effects\n",
        "    train(model, loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the training results, we can observe the following:\n",
        "\n",
        "**Before Quantization:**\n",
        "\n",
        "The loss decreases steadily over 10 epochs, showing that the model is learning effectively and improving its predictions.\n",
        "The final loss after 10 epochs is 0.416, indicating that the model has managed to capture the patterns in the dataset reasonably well.\n",
        "\n",
        "**After Quantization:**\n",
        "\n",
        "After applying vector quantization, the loss starts at 0.410, which is slightly lower than the final loss before quantization, suggesting that the quantization has not significantly impacted the model's immediate performance.\n",
        "Over the next 10 epochs, the loss continues to decrease, though at a slightly slower rate compared to before quantization.\n",
        "\n",
        "The final loss after quantization and additional training is 0.336, which is lower than the final loss before quantization.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "The quantization did not significantly degrade the model's performance. In fact, after retraining, the model achieved even better results than before quantization.\n",
        "\n",
        "This demonstrates the effectiveness of vector quantization as a method to compress the model while retaining (or in this case, slightly improving) its performance."
      ],
      "metadata": {
        "id": "LkMNmDbc4CfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Independent Layer Quantization:**"
      ],
      "metadata": {
        "id": "v_ddd7Nw8QSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_moons\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 1. Simple Neural Network in PyTorch\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# 2. Generate a simple dataset (two moons)\n",
        "def generate_data():\n",
        "    X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
        "    X, y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "    return X, y\n",
        "\n",
        "# 3. Train function for the simple neural network\n",
        "def train(model, loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(loader)}\")\n",
        "\n",
        "# 4. Function to perform Vector Quantization (VQ)\n",
        "def vector_quantization(weights, num_clusters=8):\n",
        "    shape = weights.shape\n",
        "    weights_flat = weights.reshape(-1, 1)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(weights_flat)\n",
        "    quantized_weights = kmeans.cluster_centers_[kmeans.labels_]\n",
        "\n",
        "    return quantized_weights.reshape(shape)\n",
        "\n",
        "# 5. Function to quantize specific layers independently\n",
        "def apply_layerwise_quantization(model, layer_quantization_params):\n",
        "    with torch.no_grad():\n",
        "        for i, (name, param) in enumerate(model.named_parameters()):\n",
        "            if len(param.shape) > 1:  # Only quantize weights (not biases)\n",
        "                if name in layer_quantization_params:  # Quantize only specified layers\n",
        "                    num_clusters = layer_quantization_params[name]\n",
        "                    quantized_weights = vector_quantization(param.numpy(), num_clusters)\n",
        "                    param.copy_(torch.tensor(quantized_weights))\n",
        "\n",
        "# 6. Main script\n",
        "def main():\n",
        "    # Generate dataset and create DataLoader\n",
        "    X, y = generate_data()\n",
        "    dataset = TensorDataset(X, y)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    model = SimpleNN()\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"Training the model before quantization:\")\n",
        "    train(model, loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "    # Define quantization parameters for each layer\n",
        "    # fc1: 8 clusters, fc2: 16 clusters, fc3: 4 clusters\n",
        "    layer_quantization_params = {\n",
        "        'fc1.weight': 8,\n",
        "        'fc2.weight': 16,\n",
        "        'fc3.weight': 4\n",
        "    }\n",
        "\n",
        "    # Apply layer-wise quantization\n",
        "    print(\"\\nApplying layer-wise quantization...\")\n",
        "    apply_layerwise_quantization(model, layer_quantization_params)\n",
        "\n",
        "    print(\"Training the model after layer-wise quantization:\")\n",
        "    train(model, loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edkvBxpD4V_I",
        "outputId": "dd4f0aba-2a4e-4a07-abce-11bff3416864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model before quantization:\n",
            "Epoch 1, Loss: 0.661501893773675\n",
            "Epoch 2, Loss: 0.6172461025416851\n",
            "Epoch 3, Loss: 0.5786951873451471\n",
            "Epoch 4, Loss: 0.5380938546732068\n",
            "Epoch 5, Loss: 0.506526131182909\n",
            "Epoch 6, Loss: 0.4669571053236723\n",
            "Epoch 7, Loss: 0.42523473780602217\n",
            "Epoch 8, Loss: 0.3854773547500372\n",
            "Epoch 9, Loss: 0.3539395099505782\n",
            "Epoch 10, Loss: 0.3242013403214514\n",
            "\n",
            "Applying layer-wise quantization...\n",
            "Training the model after layer-wise quantization:\n",
            "Epoch 1, Loss: 0.29677776945754886\n",
            "Epoch 2, Loss: 0.2770363623276353\n",
            "Epoch 3, Loss: 0.25871247006580234\n",
            "Epoch 4, Loss: 0.24440406332723796\n",
            "Epoch 5, Loss: 0.2334425817243755\n",
            "Epoch 6, Loss: 0.22327196062542498\n",
            "Epoch 7, Loss: 0.21568034356459975\n",
            "Epoch 8, Loss: 0.20416234119329602\n",
            "Epoch 9, Loss: 0.1966654050629586\n",
            "Epoch 10, Loss: 0.1880769394338131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Illustration of the Process:**\n",
        "\n",
        "Let’s assume we have 3 weights: 0.52, 1.3, and -0.9.\n",
        "\n",
        "**Step 1: Cluster the Weights**\n",
        "\n",
        "Assume we perform K-Means clustering and obtain the following 3 centroids:\n",
        "\n",
        "Centroid 1: 0.5\n",
        "\n",
        "Centroid 2: 1.25\n",
        "\n",
        "Centroid 3: -1.0\n",
        "\n",
        "The lookup table (codebook) would look like:\n",
        "\n",
        "Lookup Table (Codebook):\n",
        "Index | Centroid\n",
        "----------------\n",
        "\n",
        "  1   |   0.5\n",
        "\n",
        "  2   |   1.25\n",
        "  \n",
        "  3   |  -1.0\n",
        "\n",
        "**Step 2: Quantize the Weights**\n",
        "\n",
        "Each weight is replaced with the index of its nearest centroid:\n",
        "\n",
        "0.52 is closest to centroid 0.5 → Replace 0.52 with index 1\n",
        "1.3 is closest to centroid 1.25 → Replace 1.3 with index 2\n",
        "-0.9 is closest to centroid -1.0 → Replace -0.9 with index 3\n",
        "So, the weights [0.52, 1.3, -0.9] are replaced by the indices [1, 2, 3].\n",
        "\n",
        "**Step 3: Use Lookup Table During Inference**\n",
        "\n",
        "When the model uses these weights during inference:\n",
        "\n",
        "It sees index 1 and looks up 0.5 in the codebook.\n",
        "It sees index 2 and looks up 1.25 in the codebook.\n",
        "It sees index 3 and looks up -1.0 in the codebook.\n",
        "So, even though the original weights are not stored in the model, the model can reconstruct approximate versions of them using the lookup table."
      ],
      "metadata": {
        "id": "4284RiP_6PUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lookup Table Illustration**"
      ],
      "metadata": {
        "id": "KqDewRE58kBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example centroids (lookup table)\n",
        "centroids = np.array([0.5, 1.25, -1.0])\n",
        "\n",
        "# Quantized weights (indices referring to centroids)\n",
        "quantized_weights = np.array([1, 2, 3])\n",
        "\n",
        "# Lookup table to retrieve the actual weights\n",
        "def lookup_weights(quantized_weights, centroids):\n",
        "    # Subtract 1 because index is 1-based (in our example), but arrays are 0-based\n",
        "    actual_weights = centroids[quantized_weights - 1]\n",
        "    return actual_weights\n",
        "\n",
        "# Get the dequantized (approximate) weights\n",
        "dequantized_weights = lookup_weights(quantized_weights, centroids)\n",
        "print(dequantized_weights)  # Output: [0.5 1.25 -1.0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIyqo_x46IHK",
        "outputId": "88fc66d8-aa45-4c37-aa51-08a1861c9a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.5   1.25 -1.  ]\n"
          ]
        }
      ]
    }
  ]
}