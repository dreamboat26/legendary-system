{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Cat Facter Agent**\n"
      ],
      "metadata": {
        "id": "5truC6_KzUUM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYaiTV8XzGVi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datetime import datetime\n",
        "\n",
        "class SwarmBrain(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"HuggingFaceTB/SmolLM2-135M\"):\n",
        "        super(SwarmBrain, self).__init__()\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.bfloat16\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def make_decision(self, context):\n",
        "        prompt = f\"\"\"As an AI swarm coordinator, analyze this context and make a decision:\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Decision needed: What should the swarm do next?\n",
        "\n",
        "        Response format:\n",
        "        Action: [specific action to take]\n",
        "        Reason: [explanation]\n",
        "        Parameters: [any specific parameters needed]\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        outputs = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=200,\n",
        "            temperature=0.7,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True\n",
        "        )\n",
        "\n",
        "        decision = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        confidence = torch.mean(torch.stack([torch.max(score) for score in outputs.scores])).item()\n",
        "\n",
        "        return decision, confidence\n",
        "\n",
        "class SwarmAgent:\n",
        "    def __init__(self, api_url):\n",
        "        self.api_url = api_url\n",
        "\n",
        "    def get_cat_fact(self):\n",
        "        \"\"\"Make an API call to retrieve a random cat fact\"\"\"\n",
        "        try:\n",
        "            response = requests.get(self.api_url)\n",
        "            if response.status_code == 200:\n",
        "                return response.json().get(\"data\")[0]  # Extract the first cat fact\n",
        "            return \"No cat fact retrieved.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "class CatFactCollector:\n",
        "    def __init__(self, api_url, num_facts=3):\n",
        "        self.agents = [SwarmAgent(api_url) for _ in range(num_facts)]\n",
        "\n",
        "    def get_cat_facts(self):\n",
        "        \"\"\"Retrieve a list of cat facts\"\"\"\n",
        "        facts = [agent.get_cat_fact() for agent in self.agents]\n",
        "        return facts\n",
        "\n",
        "    def save_facts_to_notepad(self, facts):\n",
        "        \"\"\"Save the cat facts to a Notepad file\"\"\"\n",
        "        filename = f\"CatFacts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "        with open(filename, \"w\") as file:\n",
        "            file.write(\"Cat Facts:\\n\\n\")\n",
        "            for i, fact in enumerate(facts, 1):\n",
        "                file.write(f\"{i}. {fact}\\n\")\n",
        "        print(f\"Cat facts saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL for the cat facts API\n",
        "    api_url = \"https://meowfacts.herokuapp.com/\"\n",
        "\n",
        "    # Initialize the cat fact collector and get facts\n",
        "    collector = CatFactCollector(api_url)\n",
        "    facts = collector.get_cat_facts()\n",
        "\n",
        "    # Save the facts to a Notepad file\n",
        "    collector.save_facts_to_notepad(facts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bored Agent**"
      ],
      "metadata": {
        "id": "zZciT0Buzfnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import requests\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "class SwarmBrain(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"HuggingFaceTB/SmolLM2-135M\"):\n",
        "        super(SwarmBrain, self).__init__()\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.bfloat16\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def make_decision(self, context):\n",
        "        prompt = f\"\"\"As an AI swarm coordinator, analyze this context and make a decision:\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Decision needed: What should the swarm do next?\n",
        "\n",
        "        Response format:\n",
        "        Action: [specific action to take]\n",
        "        Reason: [explanation]\n",
        "        Parameters: [any specific parameters needed]\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        outputs = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=200,\n",
        "            temperature=0.7,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True\n",
        "        )\n",
        "\n",
        "        decision = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        confidence = torch.mean(torch.stack([torch.max(score) for score in outputs.scores])).item()\n",
        "\n",
        "        return decision, confidence\n",
        "\n",
        "    def generate_essay(self, activities):\n",
        "        \"\"\"Generate a short essay about the activities\"\"\"\n",
        "        context = \"Here are some interesting activities to try:\\n\\n\" + \"\\n\".join(activities)\n",
        "        prompt = f\"\"\"Write a short essay about the following activities and their potential benefits:\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Essay:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        outputs = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=500,\n",
        "            temperature=0.7,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True\n",
        "        )\n",
        "\n",
        "        essay = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        return essay\n",
        "\n",
        "class SwarmAgent:\n",
        "    def __init__(self, api_url, retries=3):\n",
        "        self.api_url = api_url\n",
        "        self.retries = retries\n",
        "\n",
        "    def get_activity(self):\n",
        "        \"\"\"Make an API call to retrieve a random activity with retries\"\"\"\n",
        "        for attempt in range(self.retries):\n",
        "            try:\n",
        "                response = requests.get(self.api_url, timeout=5)\n",
        "                if response.status_code == 200:\n",
        "                    return response.json().get(\"activity\", \"No activity available.\")\n",
        "                else:\n",
        "                    print(f\"Attempt {attempt+1} failed with status code {response.status_code}. Retrying...\")\n",
        "            except requests.RequestException as e:\n",
        "                print(f\"Attempt {attempt+1} encountered an error: {e}. Retrying...\")\n",
        "            time.sleep(1)  # wait before retrying\n",
        "        return \"No activity retrieved after multiple attempts.\"\n",
        "\n",
        "class ActivityCollector:\n",
        "    def __init__(self, api_url, num_activities=3):\n",
        "        self.agents = [SwarmAgent(api_url) for _ in range(num_activities)]\n",
        "        self.brain = SwarmBrain()\n",
        "\n",
        "    def get_activities(self):\n",
        "        \"\"\"Retrieve a list of random activities\"\"\"\n",
        "        activities = [agent.get_activity() for agent in self.agents]\n",
        "        return activities\n",
        "\n",
        "    def save_activities_to_notepad(self, activities):\n",
        "        \"\"\"Save the activities and a generated essay to a Notepad file\"\"\"\n",
        "        essay = self.brain.generate_essay(activities)\n",
        "        filename = f\"Activities_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "\n",
        "        with open(filename, \"w\") as file:\n",
        "            file.write(\"Suggested Activities:\\n\\n\")\n",
        "            for i, activity in enumerate(activities, 1):\n",
        "                file.write(f\"{i}. {activity}\\n\")\n",
        "            file.write(\"\\n\\nGenerated Essay:\\n\\n\")\n",
        "            file.write(essay)\n",
        "\n",
        "        print(f\"Activities and essay saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL for the alternative Bored API\n",
        "    api_url = \"https://bored-api.appbrewery.com/random\"\n",
        "\n",
        "    # Initialize the activity collector and get activities\n",
        "    collector = ActivityCollector(api_url)\n",
        "    activities = collector.get_activities()\n",
        "\n",
        "    # Save the activities and the essay to a Notepad file\n",
        "    collector.save_activities_to_notepad(activities)"
      ],
      "metadata": {
        "id": "lpZ1WGROzf6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi Agent Story Generator**"
      ],
      "metadata": {
        "id": "ZUGLDWbczreB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "class SwarmBrain(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"HuggingFaceTB/SmolLM2-135M\"):\n",
        "        super(SwarmBrain, self).__init__()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name\n",
        "        ).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        # Ensure pad token is set\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def integrate_story(self, contributions, title):\n",
        "        \"\"\"Integrate agents' contributions into a coherent story.\"\"\"\n",
        "        contributions_text = \"\\n\\n\".join(contributions)\n",
        "        prompt = f\"\"\"Title: {title}\n",
        "\n",
        "Agents' Contributions:\n",
        "{contributions_text}\n",
        "\n",
        "As the SwarmBrain, integrate the above contributions into a coherent and engaging story. Ensure the story flows well, characters are consistent, and the plot is compelling.\n",
        "\n",
        "Final Story:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        outputs = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=500,\n",
        "            temperature=0.7,\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        story = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract the final story after \"Final Story:\"\n",
        "        story = story.split(\"Final Story:\")[-1].strip()\n",
        "        return story\n",
        "\n",
        "class SwarmAgent:\n",
        "    def __init__(self, agent_id, model_name=\"HuggingFaceTB/SmolLM2-135M\"):\n",
        "        self.agent_id = agent_id\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name\n",
        "        ).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        # Ensure pad token is set\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def write_contribution(self, context):\n",
        "        \"\"\"Agent writes a paragraph based on the provided context.\"\"\"\n",
        "        prompt = f\"\"\"As Agent {self.agent_id}, continue the story based on the context below. Write a cohesive and engaging paragraph that builds upon the existing narrative.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Agent {self.agent_id}'s Contribution:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        outputs = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            early_stopping=True,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        contribution = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract the agent's contribution after \"Agent X's Contribution:\"\n",
        "        contribution = contribution.split(f\"Agent {self.agent_id}'s Contribution:\")[-1].strip()\n",
        "        return contribution\n",
        "\n",
        "class StoryCoordinator:\n",
        "    def __init__(self, num_agents=3):\n",
        "        self.agents = [SwarmAgent(agent_id=i+1) for i in range(num_agents)]\n",
        "        self.brain = SwarmBrain()\n",
        "        self.title = self.generate_story_title()\n",
        "\n",
        "    def generate_story_title(self):\n",
        "        \"\"\"Generate a random story title.\"\"\"\n",
        "        titles = [\n",
        "            \"The Lost Artifact\",\n",
        "            \"Echoes of the Past\",\n",
        "            \"Journey to the Unknown\",\n",
        "            \"Shadows in the Forest\",\n",
        "            \"The Time Traveler's Dilemma\"\n",
        "        ]\n",
        "        return random.choice(titles)\n",
        "\n",
        "    def coordinate_story(self):\n",
        "        \"\"\"Coordinate agents to create a story.\"\"\"\n",
        "        context = f\"Title: {self.title}\\n\\nOnce upon a time,\"\n",
        "        contributions = []\n",
        "\n",
        "        for agent in self.agents:\n",
        "            contribution = agent.write_contribution(context)\n",
        "            contributions.append(contribution)\n",
        "            # Update context for the next agent\n",
        "            context += \"\\n\\n\" + contribution\n",
        "\n",
        "        final_story = self.brain.integrate_story(contributions, self.title)\n",
        "        return final_story\n",
        "\n",
        "    def save_story(self, story):\n",
        "        \"\"\"Save the final story to a file.\"\"\"\n",
        "        filename = f\"Story_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "        with open(filename, \"w\", encoding='utf-8') as file:\n",
        "            file.write(f\"Title: {self.title}\\n\\n\")\n",
        "            file.write(story)\n",
        "        print(f\"Story saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the story coordinator\n",
        "    coordinator = StoryCoordinator(num_agents=3)\n",
        "\n",
        "    # Coordinate the agents to create a story\n",
        "    final_story = coordinator.coordinate_story()\n",
        "\n",
        "    # Save the final story to a file\n",
        "    coordinator.save_story(final_story)"
      ],
      "metadata": {
        "id": "m-g2no5OzyF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}