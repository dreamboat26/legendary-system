{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqh07CgN0iYq",
        "outputId": "d2ed86e7-113f-4a5c-84f5-f287c87695b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Collecting datasets>=2.21.0 (from trl)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.3)\n",
            "Collecting transformers>=4.46.0 (from trl)\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (4.66.6)\n",
            "Collecting xxhash (from datasets>=2.21.0->trl)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.9.11)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers>=4.46.0->trl)\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.21.0->trl) (0.2.0)\n",
            "Downloading trl-0.12.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, tokenizers, transformers, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 tokenizers-0.20.1 transformers-4.46.1 trl-0.12.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install trl accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "from trl.models import AutoModelForCausalLMWithValueHead\n",
        "from trl.core import set_seed\n",
        "import os\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Specify the model checkpoint and create output directory\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "output_dir = \"./ppo_logs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load the tokenizer and model, then wrap it with a value head\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# Set pad token to eos token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ").to(device)\n",
        "\n",
        "# Wrap the model with a value head for PPO\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model).to(device)\n",
        "\n",
        "# Define batch sizes\n",
        "BATCH_SIZE = 2  # Smaller batch size for easier handling\n",
        "\n",
        "# Define PPO configuration\n",
        "ppo_config = PPOConfig(\n",
        "    learning_rate=1e-5,\n",
        "    mini_batch_size=1,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Initialize PPO trainer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=None\n",
        ")\n",
        "\n",
        "# Reward function (example) - Customize this based on your criteria\n",
        "def reward_function(prompt, response):\n",
        "    # Dummy example: reward longer responses, customize as needed\n",
        "    reward = len(response) / 100\n",
        "    return reward\n",
        "\n",
        "# Training loop\n",
        "prompts = [\"Explain gravity\", \"How does photosynthesis work?\", \"What is quantum computing?\"]\n",
        "for epoch in range(10):  # Run for multiple epochs to fine-tune\n",
        "    # Process prompts in batches\n",
        "    for i in range(0, len(prompts), BATCH_SIZE):\n",
        "        batch_prompts = prompts[i:i + BATCH_SIZE]\n",
        "\n",
        "        # If the last batch is incomplete, pad it with the first prompt\n",
        "        while len(batch_prompts) < BATCH_SIZE:\n",
        "            batch_prompts.append(prompts[0])\n",
        "\n",
        "        query_tensors = []\n",
        "        response_tensors = []\n",
        "        reward_tensors = []\n",
        "\n",
        "        # Process each prompt in the batch\n",
        "        for prompt in batch_prompts:\n",
        "            # Encode the prompt with attention mask\n",
        "            query_tensor = tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_attention_mask=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Generate response with the model\n",
        "            response_ids = model.generate(\n",
        "                query_tensor.input_ids,\n",
        "                attention_mask=query_tensor.attention_mask,\n",
        "                max_length=100,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "            # Decode response\n",
        "            response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "\n",
        "            # Encode response\n",
        "            response_tensor = tokenizer(\n",
        "                response,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_attention_mask=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Calculate reward\n",
        "            reward_value = reward_function(prompt, response)\n",
        "            reward_tensor = torch.tensor([reward_value], device=device)\n",
        "\n",
        "            # Append to batch lists\n",
        "            query_tensors.append(query_tensor.input_ids[0])\n",
        "            response_tensors.append(response_tensor.input_ids[0])\n",
        "            reward_tensors.append(reward_tensor)\n",
        "\n",
        "        # Run PPO step to update the model with the accumulated batch\n",
        "        ppo_trainer.step(\n",
        "            query_tensors,\n",
        "            response_tensors,\n",
        "            reward_tensors\n",
        "        )\n",
        "\n",
        "        # Save the model periodically\n",
        "        if epoch % 5 == 0:\n",
        "            model.save_pretrained(os.path.join(output_dir, f\"checkpoint-{epoch}\"))\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed\")\n",
        "\n",
        "# Test the fine-tuned model\n",
        "test_prompt = \"Gravity is\"\n",
        "test_inputs = tokenizer(\n",
        "    test_prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_attention_mask=True\n",
        ").to(device)\n",
        "\n",
        "output_ids = model.generate(\n",
        "    test_inputs.input_ids,\n",
        "    attention_mask=test_inputs.attention_mask,\n",
        "    max_length=100,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output)\n",
        "\n",
        "# Save the final model\n",
        "model.save_pretrained(os.path.join(output_dir, \"final_model\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO_8Yve10kUP",
        "outputId": "b856ca81-074e-43b6-91f0-60d847126b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -3.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -3.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.10 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed\n",
            "Epoch 9 completed\n",
            "Epoch 10 completed\n",
            "Gravity is a force that pulls objects towards each other. It is the force that keeps us on the ground.\n",
            "\n",
            "The Earth is a sphere, so the force of gravity is the same on all objects.\n",
            "\n",
            "The Earth is a sphere, so the force of gravity is the same on all objects.\n",
            "\n",
            "The Earth is a sphere, so the force of gravity is the same on all objects.\n",
            "\n",
            "The Earth is a sphere, so the force of gravity is the same on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "from ale_py import ALEInterface, roms\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# Define Gaussian Probability Layer (GPL) for probabilistic \"twistronics\" effect\n",
        "class GaussianProbabilityLayer(nn.Module):\n",
        "    def __init__(self, std_dev=0.1):\n",
        "        super(GaussianProbabilityLayer, self).__init__()\n",
        "        self.std_dev = std_dev\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            noise = torch.randn_like(x) * self.std_dev\n",
        "            return x + noise\n",
        "        return x\n",
        "\n",
        "# Define NoisyLinear for exploration-exploitation tradeoff\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std_init=0.5):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
        "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
        "        self.std_init = std_init\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1 / np.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
        "\n",
        "    def reset_noise(self):\n",
        "        epsilon_in = self._scale_noise(self.in_features)\n",
        "        epsilon_out = self._scale_noise(self.out_features)\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.training:\n",
        "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
        "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
        "        else:\n",
        "            weight = self.weight_mu\n",
        "            bias = self.bias_mu\n",
        "        return nn.functional.linear(input, weight, bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def _scale_noise(size):\n",
        "        x = torch.randn(size)\n",
        "        return x.sign().mul_(x.abs().sqrt_())\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Dynamically determine the output size of the convolutional layers\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        # Apply GaussianProbabilityLayer between convolutional and fully connected layers\n",
        "        self.gpl1 = GaussianProbabilityLayer(std_dev=0.1)\n",
        "\n",
        "        # Value stream\n",
        "        self.fc_value = nn.Sequential(\n",
        "            NoisyLinear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            self.gpl1,\n",
        "            NoisyLinear(512, 1)\n",
        "        )\n",
        "\n",
        "        # Advantage stream\n",
        "        self.fc_advantage = nn.Sequential(\n",
        "            NoisyLinear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            GaussianProbabilityLayer(std_dev=0.05),\n",
        "            NoisyLinear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        # Generate a dummy input to pass through conv layers and calculate output size\n",
        "        dummy_input = torch.zeros(1, *shape)\n",
        "        o = self.conv(dummy_input)\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Check the input shape\n",
        "        print(f\"Input shape to DuelingDQN forward: {x.shape}\")\n",
        "\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "\n",
        "        # Print the shape of conv_out for debugging\n",
        "        print(f\"Shape after conv layers (conv_out): {conv_out.shape}\")\n",
        "\n",
        "        conv_out = self.gpl1(conv_out)\n",
        "        value = self.fc_value(conv_out)\n",
        "        advantage = self.fc_advantage(conv_out)\n",
        "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "\n",
        "# Define the Experience tuple for replay buffer\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "# Define the Prioritized Replay Buffer\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.alpha = alpha\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.priorities = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        max_priority = max(self.priorities, default=1.0)\n",
        "        self.buffer.append(Experience(state, action, reward, next_state, done))\n",
        "        self.priorities.append(float(max_priority))\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return None\n",
        "\n",
        "        priorities = np.array(list(self.priorities), dtype=np.float32)\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        indices = random.choices(range(len(self.buffer)), k=batch_size, weights=probabilities)\n",
        "        experiences = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        states = torch.stack([exp.state for exp in experiences])\n",
        "        actions = torch.tensor([exp.action for exp in experiences], dtype=torch.long)\n",
        "        rewards = torch.tensor([exp.reward for exp in experiences], dtype=torch.float)\n",
        "        next_states = torch.stack([exp.next_state for exp in experiences])\n",
        "        dones = torch.tensor([exp.done for exp in experiences], dtype=torch.float)\n",
        "        weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = float(priority.item())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Preprocess the state from the environment\n",
        "def preprocess_state(state):\n",
        "    gray = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
        "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "    processed = torch.tensor(resized, dtype=torch.float32).unsqueeze(0) / 255.0\n",
        "    return processed\n",
        "\n",
        "# Define the Swarm Member for individual DQN agents\n",
        "class SwarmMember:\n",
        "    def __init__(self, state_shape, n_actions, device, id):\n",
        "        self.id = id\n",
        "        self.device = device\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.policy_net = DuelingDQN(state_shape, n_actions).to(device)\n",
        "        self.target_net = DuelingDQN(state_shape, n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0001)\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = 0.9995\n",
        "\n",
        "        self.personal_best_reward = float('-inf')\n",
        "        self.personal_best_weights = copy.deepcopy(self.policy_net.state_dict())\n",
        "\n",
        "    def update_personal_best(self, episode_reward):\n",
        "        if episode_reward > self.personal_best_reward:\n",
        "            self.personal_best_reward = episode_reward\n",
        "            self.personal_best_weights = copy.deepcopy(self.policy_net.state_dict())\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def get_suggested_action(self, state):\n",
        "        # Epsilon-greedy policy for exploration\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).argmax(dim=1).item()\n",
        "\n",
        "# Define the Swarm DQN with Qbert environment\n",
        "class SwarmDQN:\n",
        "    def __init__(self, state_shape, n_actions, swarm_size=5, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.device = device\n",
        "        self.swarm_size = swarm_size\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.swarm = [SwarmMember(state_shape, n_actions, device, i) for i in range(swarm_size)]\n",
        "\n",
        "        self.memory = PrioritizedReplayBuffer(100000)\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        self.global_best_reward = float('-inf')\n",
        "        self.global_best_weights = copy.deepcopy(self.swarm[0].policy_net.state_dict())\n",
        "\n",
        "    def select_action(self, state, member_idx):\n",
        "        member = self.swarm[member_idx]\n",
        "\n",
        "        suggested_action = member.get_suggested_action(state.squeeze().cpu())\n",
        "        if random.random() < member.epsilon:\n",
        "            if suggested_action is not None and random.random() < 0.7:\n",
        "                return suggested_action\n",
        "            return random.randrange(self.n_actions)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = member.policy_net(state)\n",
        "                if suggested_action is not None:\n",
        "                    q_values[0][suggested_action] += 0.1\n",
        "                return torch.argmax(q_values).item()\n",
        "\n",
        "    def calculate_reward(self, raw_reward, suggested_action, taken_action):\n",
        "        reward = raw_reward\n",
        "        if suggested_action is not None and suggested_action == taken_action:\n",
        "            reward += 0.1\n",
        "        return reward\n",
        "\n",
        "    def update_member(self, member_idx, batch):\n",
        "        member = self.swarm[member_idx]\n",
        "        states, actions, rewards, next_states, dones, indices, weights = batch\n",
        "\n",
        "        states = states.view(self.batch_size, -1, 84, 84).to(self.device)\n",
        "        next_states = next_states.view(self.batch_size, -1, 84, 84).to(self.device)\n",
        "\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "        weights = weights.to(self.device)\n",
        "\n",
        "        # Forward pass only\n",
        "        with torch.no_grad():\n",
        "            current_q_values = member.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "            next_actions = member.policy_net(next_states).max(1)[1]\n",
        "            next_q_values = member.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "            target_q_values = rewards + (self.gamma * next_q_values * (1.0 - dones))\n",
        "\n",
        "        td_errors = torch.abs(current_q_values - target_q_values.unsqueeze(1)).detach().cpu().numpy()\n",
        "        self.memory.update_priorities(indices, td_errors.squeeze())\n",
        "\n",
        "    def update_global_best(self, member_idx, episode_reward):\n",
        "        if episode_reward > self.global_best_reward:\n",
        "            self.global_best_reward = episode_reward\n",
        "            self.global_best_weights = copy.deepcopy(self.swarm[member_idx].policy_net.state_dict())\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Train the Swarm Agent for Qbert\n",
        "def train_swarm_agent(episodes=1000, render_frequency=100, score_threshold=50, hit_threshold=20):\n",
        "    ale = ALEInterface()\n",
        "    ale.setInt('random_seed', 123)\n",
        "    ale.setBool('sound', False)\n",
        "    ale.setBool('display_screen', True)\n",
        "    ale.setFloat('repeat_action_probability', 0.0)\n",
        "    ale.loadROM(roms.get_rom_path(\"qbert\"))\n",
        "\n",
        "    actions = ale.getMinimalActionSet()\n",
        "    state_shape = (4, 84, 84)\n",
        "    swarm = SwarmDQN(state_shape, len(actions))\n",
        "\n",
        "    episode_rewards = []\n",
        "    swarm_rewards = [[] for _ in range(swarm.swarm_size)]\n",
        "\n",
        "    for episode in tqdm(range(1, episodes + 1)):\n",
        "        member_idx = random.randrange(swarm.swarm_size)\n",
        "        member = swarm.swarm[member_idx]\n",
        "\n",
        "        ale.reset_game()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        state_stack = deque([preprocess_state(ale.getScreenRGB()) for _ in range(4)], maxlen=4)\n",
        "\n",
        "        while not done:\n",
        "            stacked_state = torch.cat(list(state_stack), dim=0).unsqueeze(0).to(swarm.device)\n",
        "\n",
        "            action_idx = swarm.select_action(stacked_state, member_idx)\n",
        "            reward = 0\n",
        "\n",
        "            for _ in range(4):\n",
        "                reward += ale.act(actions[action_idx])\n",
        "                if ale.game_over():\n",
        "                    done = True\n",
        "                    break\n",
        "\n",
        "            next_state = preprocess_state(ale.getScreenRGB()) if not done else state_stack[-1].clone()\n",
        "            state_stack.append(next_state)\n",
        "\n",
        "            stacked_next_state = torch.cat(list(state_stack), dim=0).unsqueeze(0).to(swarm.device)\n",
        "\n",
        "            swarm.memory.push(stacked_state, action_idx, float(reward), stacked_next_state, float(done))\n",
        "\n",
        "            if len(swarm.memory) >= swarm.batch_size:\n",
        "                batch = swarm.memory.sample(swarm.batch_size)\n",
        "                if batch is not None:\n",
        "                    swarm.update_member(member_idx, batch)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if episode % render_frequency == 0:\n",
        "                screen = ale.getScreenRGB()\n",
        "                cv2.imshow('Qbert', cv2.cvtColor(screen, cv2.COLOR_RGB2BGR))\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        swarm_rewards[member_idx].append(total_reward)\n",
        "\n",
        "        personal_best_updated = member.update_personal_best(total_reward)\n",
        "        if personal_best_updated:\n",
        "            swarm.update_global_best(member_idx, total_reward)\n",
        "\n",
        "        member.epsilon = max(member.epsilon_min, member.epsilon * member.epsilon_decay)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, Member {member_idx} Epsilon: {member.epsilon:.2f}\")\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    return swarm, episode_rewards, swarm_rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        swarm, episode_rewards, swarm_rewards = train_swarm_agent()\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(episode_rewards)\n",
        "        plt.title(\"Overall Training Progress\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        for i, rewards in enumerate(swarm_rewards):\n",
        "            plt.plot(rewards, label=f\"Member {i}\")\n",
        "        plt.title(\"Individual Swarm Member Performance\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "6T3AuVMQydP8",
        "outputId": "4d53cbe9-7cb9-4dfb-9508-27938111caf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'modules'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3a51a531344c>\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Initialize Custom PPO trainer with the custom model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m ppo_trainer = CustomPPOTrainer(\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mppo_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_with_v_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-3a51a531344c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCustomPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPOTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Override disable_dropout_in_model to prevent it from running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_dropout_in_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, processing_class, policy, ref_policy, reward_model, train_dataset, value_model, data_collator, eval_dataset, optimizers, callbacks)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m#########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mdisable_dropout_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_token\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_token\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eos\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py\u001b[0m in \u001b[0;36mdisable_dropout_in_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisable_dropout_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'modules'"
          ]
        }
      ]
    }
  ]
}