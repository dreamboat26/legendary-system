{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGzt4EFV5h4_",
        "outputId": "ff0e3ce0-7b97-4a5f-8df1-86c01c5b5b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.1563308238983154\n",
            "Epoch 2, Loss: 1.2336087226867676\n",
            "Epoch 3, Loss: 0.9677525758743286\n",
            "Epoch 4, Loss: 0.8561504483222961\n",
            "Epoch 5, Loss: 0.7402212619781494\n",
            "Epoch 6, Loss: 0.735029935836792\n",
            "Epoch 7, Loss: 0.6182444095611572\n",
            "Epoch 8, Loss: 0.6720882654190063\n",
            "Epoch 9, Loss: 0.6195396184921265\n",
            "Epoch 10, Loss: 0.5230082869529724\n",
            "Test Accuracy: 82.22%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Kolmogorov-Arnold Network Layer Definition\n",
        "class KANLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(KANLayer, self).__init__()\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.activation(x)\n",
        "        return self.output(x)\n",
        "\n",
        "# Entropy-Based Sampling for Batch\n",
        "def entropy_based_sampling_batch(logits):\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-8), dim=-1)\n",
        "    return entropy\n",
        "\n",
        "# Model Combining KAN and Entropix\n",
        "class EntropixKANModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(EntropixKANModel, self).__init__()\n",
        "        self.kan_layer = KANLayer(input_dim, hidden_dim, hidden_dim)\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process entire batch through the KAN layer\n",
        "        kan_output = self.kan_layer(x)\n",
        "\n",
        "        # Apply entropy-based sampling to the entire batch\n",
        "        entropy = entropy_based_sampling_batch(kan_output)\n",
        "\n",
        "        # Use the entropy values to weight the final layer's output\n",
        "        weighted_output = self.output_layer(kan_output) * entropy.unsqueeze(1)\n",
        "        return weighted_output\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = EntropixKANModel(input_dim=4, hidden_dim=16, output_dim=3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kolmogorov-Arnold Network + Entropy Based Sampling = ^"
      ],
      "metadata": {
        "id": "3fQOZ5aQS7Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standard Neural Network Definition\n",
        "class StandardNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(StandardNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.layer1(x))\n",
        "        return self.output(x)\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = StandardNN(input_size=4, hidden_size=16, output_size=3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JViByAESXeD",
        "outputId": "be6840a6-1d81-46c2-c0e5-d24ac1dcb2b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.0459\n",
            "Epoch 2, Loss: 1.0154\n",
            "Epoch 3, Loss: 0.9919\n",
            "Epoch 4, Loss: 0.9706\n",
            "Epoch 5, Loss: 0.9406\n",
            "Epoch 6, Loss: 0.9104\n",
            "Epoch 7, Loss: 0.8802\n",
            "Epoch 8, Loss: 0.8639\n",
            "Epoch 9, Loss: 0.8425\n",
            "Epoch 10, Loss: 0.8142\n",
            "Test Accuracy: 71.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Neural Network = ^"
      ],
      "metadata": {
        "id": "iKob5vOwTH7G"
      }
    }
  ]
}